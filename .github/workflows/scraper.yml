name: EV News Scraper

on:
  schedule:
    - cron: '0 */6 * * *'  # Every 6 hours (0, 6, 12, 18 UTC)
  workflow_dispatch:  # Manual trigger

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
      - name: Log workflow start
        run: |
          echo "=========================================="
          echo "EV News Scraper - GitHub Actions"
          echo "=========================================="
          echo "Triggered by: ${{ github.event_name }}"
          echo "Run ID: ${{ github.run_id }}"
          echo "Run number: ${{ github.run_number }}"
          echo "Repository: ${{ github.repository }}"
          echo "Branch: ${{ github.ref_name }}"
          echo "Commit: ${{ github.sha }}"
          echo "Runner OS: ${{ runner.os }}"
          echo "Start time: $(date -u '+%Y-%m-%d %H:%M:%S UTC')"
          echo "=========================================="

      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
          cache-dependency-path: scraper/requirements.txt

      - name: Log Python environment
        run: |
          echo "Python version: $(python --version)"
          echo "Pip version: $(pip --version)"
          echo "Working directory: $(pwd)"

      - name: Install dependencies
        run: |
          echo "Installing Python dependencies..."
          cd scraper
          pip install -r requirements.txt
          echo "Dependencies installed successfully"

      - name: Install Playwright browsers
        run: |
          echo "Installing Playwright Chromium..."
          playwright install chromium
          echo "Playwright installed successfully"

      - name: Validate secrets
        run: |
          echo "Validating required secrets..."
          missing_secrets=""

          if [ -z "${{ secrets.WEBHOOK_URL }}" ]; then
            missing_secrets="$missing_secrets WEBHOOK_URL"
          else
            echo "✓ WEBHOOK_URL is set"
          fi

          if [ -z "${{ secrets.SCRAPER_WEBHOOK_SECRET }}" ]; then
            missing_secrets="$missing_secrets SCRAPER_WEBHOOK_SECRET"
          else
            echo "✓ SCRAPER_WEBHOOK_SECRET is set"
          fi

          if [ -z "${{ secrets.DEEPSEEK_API_KEY }}" ] && [ -z "${{ secrets.OPENAI_API_KEY }}" ]; then
            echo "⚠ WARNING: Neither DEEPSEEK_API_KEY nor OPENAI_API_KEY is set - AI processing will fail"
          else
            if [ -n "${{ secrets.DEEPSEEK_API_KEY }}" ]; then
              echo "✓ DEEPSEEK_API_KEY is set"
            fi
            if [ -n "${{ secrets.OPENAI_API_KEY }}" ]; then
              echo "✓ OPENAI_API_KEY is set"
            fi
          fi

          if [ -n "$missing_secrets" ]; then
            echo "❌ ERROR: Missing required secrets:$missing_secrets"
            exit 1
          fi

          echo "All required secrets validated"

      - name: Run scraper
        id: scraper
        run: |
          echo "=========================================="
          echo "Starting scraper..."
          echo "Webhook URL: $WEBHOOK_URL"
          echo "Skip X Publish: $SKIP_X_PUBLISH"
          echo "=========================================="

          # Run scraper and capture exit code
          python scraper/main.py
          exit_code=$?

          echo "=========================================="
          echo "Scraper finished with exit code: $exit_code"
          echo "End time: $(date -u '+%Y-%m-%d %H:%M:%S UTC')"
          echo "=========================================="

          exit $exit_code
        env:
          DEEPSEEK_API_KEY: ${{ secrets.DEEPSEEK_API_KEY }}
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          WEBHOOK_URL: ${{ secrets.WEBHOOK_URL }}
          SCRAPER_WEBHOOK_SECRET: ${{ secrets.SCRAPER_WEBHOOK_SECRET }}
          CRON_SECRET: ${{ secrets.CRON_SECRET }}
          SKIP_X_PUBLISH: 'true'  # X publishing handled by cron-publish.yml

      - name: Log failure details
        if: failure()
        run: |
          echo "=========================================="
          echo "❌ SCRAPER FAILED"
          echo "=========================================="
          echo "Check the logs above for error details"
          echo "Common issues:"
          echo "  - Missing or invalid API keys"
          echo "  - Network/timeout errors"
          echo "  - Webhook endpoint unreachable"
          echo "  - Source website changes"
          echo "=========================================="
